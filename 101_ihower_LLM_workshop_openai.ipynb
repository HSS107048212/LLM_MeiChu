{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HSS107048212/LLM_MeiChu/blob/main/101_ihower_LLM_workshop_openai.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "é€™ä»½ Notebook ç¤ºç¯„ OpenAI API çš„åŸºæœ¬å‘¼å«æ–¹å¼\n",
        "\n",
        "### Google Colab Tips\n",
        "\n",
        "* ç”¨ Shift+Enter å¯ä»¥åŸ·è¡Œç¨‹å¼å€å¡Š (æˆ–æ˜¯æ»‘é¼ é»å‰é¢çš„åŸ·è¡Œç¬¦è™Ÿ)\n",
        "* å¦‚æœè¦ä¿®æ”¹å­˜æª”ï¼Œéœ€è¦å…ˆé» â€œæª”æ¡ˆ\" -> \"åœ¨é›²ç«¯ç¡¬ç¢Ÿä¸­å„²å­˜å‰¯æœ¬\"ï¼Œè¤‡è£½åˆ°ä½ è‡ªå·±çš„ç›®éŒ„ä¸‹ï¼Œæ‰å¯ä»¥å„²å­˜\n"
      ],
      "metadata": {
        "id": "VaGqfWz7oUVM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## è¨­å®š OpenAI API Key è®Šæ•¸\n",
        "\n",
        "è«‹é»é–‹å·¦å´æ¬„çš„Keyç¬¦è™Ÿï¼Œå°±å¯ä»¥è¨­å®š Secretã€‚è«‹è¨­å®š openai_api_key\n",
        "\n",
        "2024/5/14 æ¢…ç«¹é»‘å®¢æ¾ å·¥ä½œåŠå­¸å“¡ç”¨:\n",
        "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\n"
      ],
      "metadata": {
        "id": "cU4r9MXgfBK8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "9h39wlZGYKSJ"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "openai_api_key = userdata.get('openai_api_key')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ğŸ”¥ğŸ”¥ğŸ”¥ æˆ‘å€‘ä¸»è¦ä½¿ç”¨ requests å’Œ json library\n",
        "\n",
        "æˆ‘çŸ¥é“æ‰€æœ‰æ•™ OpenAI çš„ notebook éƒ½ç”¨ OpenAI å®˜æ–¹å‡ºçš„ https://github.com/openai/openai-python\n",
        "\n",
        "ä½†æ˜¯é€™è£¡æˆ‘çš„ç›®æ¨™è½çœ¾åŒ…æ‹¬é Python çš„å·¥ç¨‹å¸«ï¼Œå› æ­¤æˆ‘åªç”¨æœ€æœ€åŸºæœ¬çš„ HTTP libraryï¼Œç›¸ä¿¡ä½ å¾ˆå®¹æ˜“å°±å¯ä»¥æ›¿æ›æˆä¸åŒç¨‹å¼èªè¨€çš„ HTTP libraryã€‚"
      ],
      "metadata": {
        "id": "Mz4OovRbfMIl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json # é€™æœ‰å…©å€‹æ–¹æ³• dumps (ç‰©ä»¶è½‰å­—ä¸²) è·Ÿ loads (å­—ä¸²è½‰ç‰©ä»¶)\n",
        "from pprint import pp # ç‚ºäº†å°å‡ºä¾†æ¼‚äº®"
      ],
      "metadata": {
        "id": "xFHPuW7kYuHu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OpenAI Completion API\n",
        "\n",
        "é›–ç„¶ OpenAI å¾Œä¾†ä¸å¤ªç”¨é€™ç¨®å¯«æ³•äº†ï¼Œä¸éå…¶ä»–å®¶ LLM æœ‰äº›æ˜¯é€™ç¨® prompt ç”¨æ³•ï¼Œæ‰€ä»¥é‚„æ˜¯èªè­˜ä¸€ä¸‹"
      ],
      "metadata": {
        "id": "uEyIUCn7iv-I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "payload = { \"model\": \"gpt-3.5-turbo-instruct\", \"temperature\": 0, \"prompt\": \"è¬›å€‹ç¬‘è©±\" } # å¯ä»¥æ”¹æ”¹çœ‹ prompt\n",
        "headers = { \"Authorization\": f'Bearer {openai_api_key}', \"Content-Type\": \"application/json\" }\n",
        "# ğŸ Python çš„å­—ä¸²ï¼Œè‹¥å‰é¢æœ‰ fï¼Œè¡¨ç¤ºè£¡é¢å¯ä»¥ç”¨ {variable_name} ä¾†æ›¿æ›è®Šæ•¸\n",
        "\n",
        "response = requests.post('https://api.openai.com/v1/completions', headers = headers, data = json.dumps(payload) )\n",
        "obj = json.loads(response.text) # æˆ–æ˜¯ obj = response.json()\n",
        "pp(obj)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LpVGEJMficmy",
        "outputId": "995db130-c0e8-41d5-ea84-c06574794b43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'id': 'cmpl-9NdFPvWgDE7nwbNnupRU7IVtM7Ep1',\n",
            " 'object': 'text_completion',\n",
            " 'created': 1715419439,\n",
            " 'model': 'gpt-3.5-turbo-instruct',\n",
            " 'choices': [{'text': '\\n\\næœ‰ä¸€å¤©ï¼Œå°æ˜å»è²·äº†ä¸€å€‹æ–°çš„',\n",
            "              'index': 0,\n",
            "              'logprobs': None,\n",
            "              'finish_reason': 'length'}],\n",
            " 'usage': {'prompt_tokens': 7, 'completion_tokens': 16, 'total_tokens': 23}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## å‘¼å« OpenAI Chat API\n",
        "\n",
        "prompt åƒæ•¸è®Šæˆ messages å°è©± array æ ¼å¼\n",
        "\n",
        "role æœ‰åˆ†:\n",
        "\n",
        "* system: å®šç¾©å’Œå®šèª¿æ•´å€‹ messages çš„è¡Œç‚ºä½œç”¨ï¼Œæœƒæ”¾ messages array çš„ç¬¬ä¸€å€‹\n",
        "* user: ä½ çš„è¨Šæ¯\n",
        "* assistant: AIçš„å›è¦† (ç•¶ä½ è¦å¤šè¼ªå°è©±æ™‚ï¼Œå°±éœ€è¦å°‡ä¸Šæ¬¡AIçš„å›è¦†æ”¾é€²ä¾†)\n",
        "\n",
        "æœ€ç°¡å–®çš„å½¢å¼å°±æ˜¯åªæœ‰ä¸€å€‹ user messageï¼ŒæŠŠ prompt å…¨éƒ¨éƒ½æ”¾ content è£¡é¢ï¼Œé€™æ¨£å°±ç­‰åŒæ–¼ Completion API ç”¨æ³•äº†ã€‚"
      ],
      "metadata": {
        "id": "CMJGh_q8flF0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "payload = { \"model\": \"gpt-3.5-turbo\", \"temperature\": 0,\n",
        "            \"messages\": [ { \"role\": \"user\", \"content\": \"ä½ å¥½\"} ] } # å¯ä»¥æ”¹æ”¹çœ‹ content\n",
        "headers = { \"Authorization\": f'Bearer {openai_api_key}', \"Content-Type\": \"application/json\" }\n",
        "response = requests.post('https://api.openai.com/v1/chat/completions', headers = headers, data = json.dumps(payload) )\n",
        "obj = json.loads(response.text)\n",
        "\n",
        "pp(obj)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hABW7blVYv5I",
        "outputId": "fb7d800b-c841-4755-9d4f-925ebaa2fe33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'id': 'chatcmpl-9NdFQaiBsEMad3FETSZnACWh6eCFF',\n",
            " 'object': 'chat.completion',\n",
            " 'created': 1715419440,\n",
            " 'model': 'gpt-3.5-turbo-0125',\n",
            " 'choices': [{'index': 0,\n",
            "              'message': {'role': 'assistant', 'content': 'ä½ å¥½ï¼æœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ'},\n",
            "              'logprobs': None,\n",
            "              'finish_reason': 'stop'}],\n",
            " 'usage': {'prompt_tokens': 9, 'completion_tokens': 17, 'total_tokens': 26},\n",
            " 'system_fingerprint': None}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "æ³¨æ„åˆ°å›å‚³çš„ reponse æœ‰å¯«æ˜ model æ˜¯ gpt-3-5-turbo-0125 é€™å€‹ç‰ˆæœ¬"
      ],
      "metadata": {
        "id": "XYxjvxyR6MM3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## åŒ…æˆä¸€å€‹è¼”åŠ©å‡½å¼ï¼Œä¹‹å¾Œæœƒå¸¸ç”¨"
      ],
      "metadata": {
        "id": "-NRdE74nfqNq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_completion(messages, model=\"gpt-3.5-turbo\", temperature=0, max_tokens=300):\n",
        "  payload = { \"model\": model, \"temperature\": temperature, \"messages\": messages, \"max_tokens\": max_tokens }\n",
        "\n",
        "  headers = { \"Authorization\": f'Bearer {openai_api_key}', \"Content-Type\": \"application/json\" }\n",
        "  response = requests.post('https://api.openai.com/v1/chat/completions', headers = headers, data = json.dumps(payload) )\n",
        "  obj = json.loads(response.text)\n",
        "  if response.status_code == 200 :\n",
        "    return obj[\"choices\"][0][\"message\"][\"content\"]\n",
        "  else :\n",
        "    return obj[\"error\"]"
      ],
      "metadata": {
        "id": "pUvYJ90_ZfgI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_message = \"ä»€éº¼æ˜¯ Ruby? è«‹ç”¨å°ç£ç¹é«”ä¸­æ–‡\" # å¯ä»¥æ”¹ä»»æ„å•é¡Œ\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": user_message\n",
        "    }\n",
        "]\n",
        "\n",
        "response = get_completion(messages, temperature=0)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xvF9fpvJacn3",
        "outputId": "e38a912a-57ad-4044-9566-77b099f531fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ruby æ˜¯ä¸€ç¨®å‹•æ…‹ã€é–‹æºçš„ç¨‹å¼èªè¨€ï¼Œç”±æ—¥æœ¬ç¨‹å¼è¨­è¨ˆå¸«æ¾æœ¬è¡Œå¼˜ï¼ˆMatzï¼‰æ‰€å‰µé€ ã€‚Ruby è¢«å»£æ³›æ‡‰ç”¨æ–¼ç¶²é é–‹ç™¼ã€è»Ÿé«”é–‹ç™¼ã€æ•¸æ“šåˆ†æç­‰é ˜åŸŸï¼Œå…·æœ‰ç°¡æ½”æ˜“è®€çš„èªæ³•ã€è±å¯Œçš„å…§å»ºå‡½å¼åº«å’Œå¼·å¤§çš„æ“´å……æ€§ã€‚Ruby çš„ç‰¹è‰²åŒ…æ‹¬ç‰©ä»¶å°å‘ã€å‡½æ•¸å¼ç·¨ç¨‹ã€å‹•æ…‹å‹åˆ¥ç­‰ï¼Œè¢«è­½ç‚ºä¸€ç¨®å„ªé›…è€Œå¼·å¤§çš„ç¨‹å¼èªè¨€ã€‚\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Temperature æº«åº¦\n",
        "\n",
        "å¯ä»¥èª¿çœ‹çœ‹æº«åº¦"
      ],
      "metadata": {
        "id": "5z49Hmohvm-Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_message = \"å°æ˜æ„›åƒä»€éº¼? \"\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": user_message\n",
        "    }\n",
        "]\n",
        "\n",
        "response = get_completion(messages, temperature=1)  # å¯ä»¥æ”¹çœ‹çœ‹æº«åº¦\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EqU0OYCzvr7j",
        "outputId": "431d83ce-3eca-4369-ff2b-5cd0025bd0af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "é€™å€‹æ²’æœ‰ç¢ºåˆ‡çš„ä¿¡æ¯ï¼Œç„¡æ³•ç¢ºå®šå°æ˜ç©¶ç«Ÿæ„›åƒä»€éº¼ã€‚å¯èƒ½è¦å•ä»–æœ¬äººæˆ–è€…èº«é‚Šçš„äººæ‰çŸ¥é“ã€‚\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ä½¿ç”¨ System Message\n",
        "\n",
        "åªå»ºè­°åœ¨ 0613 ç‰ˆæœ¬ä¹‹å¾Œä½¿ç”¨ï¼Œä¹‹å‰ç‰ˆæœ¬ä½ éƒ½æ”¾ user message å°±å¥½äº†\n",
        "\n",
        "OpenAI å®˜æ–¹èªª: è«‹æ³¨æ„ï¼Œgpt-3.5-turbo-0301 é€šå¸¸ä¸åƒ gpt-4-0314 æˆ– gpt-3.5-turbo-0613 é‚£æ¨£å¯†åˆ‡é—œæ³¨ system messageã€‚å› æ­¤ï¼Œå°æ–¼ gpt-3.5-turbo-0301ï¼Œæˆ‘å€‘å»ºè­°å°‡é‡è¦æŒ‡ç¤ºæ”¾åœ¨ user message ä¸­ã€‚ä¸€äº›é–‹ç™¼è€…ç™¼ç¾ï¼Œåœ¨å°è©±è®Šå¾—è¼ƒé•·æ™‚ï¼Œå°‡ system prompt æŒçºŒåœ°ç§»å‹•åˆ°å°è©±çš„æœ«å°¾å¯ä»¥é˜²æ­¢æ¨¡å‹çš„æ³¨æ„åŠ›æ¼¸æ¼¸åé›¢ã€‚(å‡ºè‡ª https://github.com/openai/openai-cookbook/blob/main/examples/How_to_format_inputs_to_ChatGPT_models.ipynb)"
      ],
      "metadata": {
        "id": "Li5om-k2IsrZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# é€™æ˜¯ completion é¢¨æ ¼(è »å¤šæ•™æä»é€™æ¨£å¯«ï¼ŒåŒ…æ‹¬ ChatGPT Prompt Engineering for Developers èª²ç¨‹)\n",
        "user_message = \"\"\"è«‹åˆ†é¡ä»¥ä¸‹æ–‡å­—æ˜¯ neutral, negative æˆ– positive\n",
        "æ–‡å­—: æˆ‘è¦ºå¾—é€™å€‹æŠ«è–©å¯¦åœ¨å¤ªå¥½åƒå•¦\n",
        "æƒ…ç·’:\n",
        "\"\"\"\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": user_message\n",
        "    }\n",
        "]\n",
        "\n",
        "response = get_completion(messages, temperature=0.7)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2TDaCNkHVnN",
        "outputId": "db067992-564a-47b1-89c8-cbecefebd019"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# å¯æ”¹æˆä½¿ç”¨ system prompt çš„é¢¨æ ¼: æŠŠæ•´é«”æŒ‡ç¤ºæ”¾åœ¨ system message\n",
        "user_message = \"\"\"\n",
        "æ–‡å­—: æˆ‘è¦ºå¾—é€™å€‹æŠ«è–©å¯¦åœ¨å¤ªå¥½åƒå•¦\n",
        "\"\"\"\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"è«‹åˆ†é¡ä»¥ä¸‹æ–‡å­—æ˜¯ neutral, negative æˆ– positive\"\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": user_message\n",
        "    }\n",
        "]\n",
        "\n",
        "response = get_completion(messages, temperature=0.7)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dvM222w_JIuh",
        "outputId": "98299786-8f18-42d1-e06b-8a7fbc51406b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## é€£çºŒå°è©±å¤š messages çš„å ´æ™¯ç¤ºç¯„\n",
        "\n",
        "ä»¥ä¸‹æ˜¯åŒä¸€å€‹å°è©±çš„é€£çºŒä¸‰è¼ªå•ç­”"
      ],
      "metadata": {
        "id": "0ukz97tqJqd2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ç¬¬ä¸€è¼ªå•ç­”\n",
        "messages=[\n",
        "      {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "      {\"role\": \"user\", \"content\": \"èª°è´å¾—2013å¹´çš„ä¸–ç•Œæ£’çƒç¶“å…¸è³½å† è»?\"},\n",
        "  ]\n",
        "\n",
        "response1 = get_completion(messages, temperature=0.3)\n",
        "print(response1)"
      ],
      "metadata": {
        "id": "bN5teaIdJqPQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e388e80-9e58-4707-dc76-77b8a2ab97d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2013å¹´çš„ä¸–ç•Œæ£’çƒç¶“å…¸è³½å† è»æ˜¯å¤šæ˜å°¼åŠ å…±å’Œåœ‹éšŠã€‚ä»–å€‘åœ¨æ±ºè³½ä¸­æ“Šæ•—äº†æ³¢å¤šé»å„éšŠï¼Œè´å¾—äº†å† è»ã€‚\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "æ¥è‘—è¦ç¹¼çºŒå°è©±æ™‚ï¼Œä½ å¾—æŠŠ AI å›è¦†çš„è¨Šæ¯ï¼Œæ”¾åœ¨ role: assistant è£¡é¢ä¸€èµ·å‚³çµ¦ OpenAIã€‚å› ç‚º API æ˜¯ç„¡ç‹€æ…‹çš„(Stateless)"
      ],
      "metadata": {
        "id": "DKCM5ERAks4i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# å»¶çºŒåŒä¸€å€‹å°è©±çš„ ç¬¬äºŒè¼ªå•ç­”\n",
        "messages=[\n",
        "      {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "      {\"role\": \"user\", \"content\": \"èª°è´å¾—2013å¹´çš„ä¸–ç•Œæ£’çƒç¶“å…¸è³½å† è»?\"}, # é€™æ˜¯ç¬¬ä¸€è¼ªçš„ user å•å¥\n",
        "      {\"role\": \"assistant\", \"content\": response1 }, # é€™æ˜¯ç¬¬ä¸€è¼ªçš„ AI å›è¦†\n",
        "      {\"role\": \"user\", \"content\": \"é‚£2017å¹´å‘¢?\"} # é€™æ˜¯ç¬¬äºŒè¼ªçš„ user å•å¥\n",
        "]\n",
        "\n",
        "response2 = get_completion(messages, temperature=0.3)\n",
        "print(response2)"
      ],
      "metadata": {
        "id": "93XE9p5Fuxl9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa0e14be-4b19-4b38-8310-3d71eea6735d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2017å¹´çš„ä¸–ç•Œæ£’çƒç¶“å…¸è³½å† è»æ˜¯ç¾åœ‹éšŠã€‚ä»–å€‘åœ¨æ±ºè³½ä¸­æ“Šæ•—äº†æ³¢å¤šé»å„éšŠï¼Œè´å¾—äº†å† è»ã€‚\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# å»¶çºŒåŒä¸€å€‹å°è©±çš„ ç¬¬ä¸‰è¼ªå•ç­”\n",
        "messages=[\n",
        "      {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "      {\"role\": \"user\", \"content\": \"èª°è´å¾—2013å¹´çš„ä¸–ç•Œæ£’çƒç¶“å…¸è³½å† è»?\"}, # é€™æ˜¯ç¬¬ä¸€è¼ªçš„ user å•å¥\n",
        "      {\"role\": \"assistant\", \"content\": response1 }, # é€™æ˜¯ç¬¬ä¸€è¼ªçš„ AI å›è¦†\n",
        "      {\"role\": \"user\", \"content\": \"é‚£2017å¹´å‘¢?\"}, # é€™æ˜¯ç¬¬äºŒè¼ªçš„ user å•å¥\n",
        "      {\"role\": \"assistant\", \"content\": response2 }, # é€™æ˜¯ç¬¬äºŒè¼ªçš„ AI å›è¦†\n",
        "      {\"role\": \"user\", \"content\": \"ç¾åœ‹éšŠè´éå¹¾æ¬¡å† è»?\"}, # é€™æ˜¯ç¬¬ä¸‰è¼ªçš„ user å•å¥\n",
        "]\n",
        "response3 = get_completion(messages, temperature=0.3)\n",
        "print(response3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mrx_x5p8iUSq",
        "outputId": "97f47489-e399-47b8-fe9f-d98b386a2558"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ç¾åœ‹éšŠåœ¨ä¸–ç•Œæ£’çƒç¶“å…¸è³½ä¸­è´å¾—äº†ä¸€æ¬¡å† è»ï¼Œå³åœ¨2017å¹´ã€‚\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### æ¨¡å‹çš„å¹»è¦ºç¾è±¡ Hallucination"
      ],
      "metadata": {
        "id": "WzPYjxCuR5qG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# å¦‚æœ ç¬¬äºŒè¼ªå•ç­”æ™‚ æ˜¯å• 2018 å¹´\n",
        "messages=[\n",
        "      {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "      {\"role\": \"user\", \"content\": \"èª°è´å¾—2013å¹´çš„ä¸–ç•Œæ£’çƒç¶“å…¸è³½å† è»?\"},\n",
        "      {\"role\": \"assistant\", \"content\": \"2013å¹´çš„ä¸–ç•Œæ£’çƒç¶“å…¸è³½å† è»æ˜¯å¤šæ˜å°¼åŠ å…±å’Œåœ‹ã€‚\"},\n",
        "      {\"role\": \"user\", \"content\": \"é‚£2018å¹´å‘¢?\"}\n",
        "]\n",
        "response = get_completion(messages, temperature=0.3)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "atQnDcZJRr2Z",
        "outputId": "e1775345-287d-4bbd-a164-5f821e64a19d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2018å¹´çš„ä¸–ç•Œæ£’çƒç¶“å…¸è³½å† è»æ˜¯ç¾åœ‹ã€‚\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# æ›ä¸€ç¨®å•æ³•\n",
        "messages=[\n",
        "      {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "      {\"role\": \"user\", \"content\": \"èª°è´å¾—2013å¹´çš„ä¸–ç•Œæ£’çƒç¶“å…¸è³½å† è»?\"},\n",
        "      {\"role\": \"assistant\", \"content\": \"2013å¹´çš„ä¸–ç•Œæ£’çƒç¶“å…¸è³½å† è»æ˜¯å¤šæ˜å°¼åŠ å…±å’Œåœ‹ã€‚\"},\n",
        "      {\"role\": \"user\", \"content\": \"é‚£2018å¹´å‘¢? å¦‚æœæ²’èˆ‰è¾¦ï¼Œè«‹å›ç­”æ²’èˆ‰è¾¦\"} # è¦å‘Šè¨´æ¨¡å‹å¯ä»¥å›ç­”ä¸çŸ¥é“\n",
        "]\n",
        "response = get_completion(messages, temperature=0.3)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NkbaWB0kR1hK",
        "outputId": "bb8b7ab1-1c19-4fec-8f70-2f4f12066219"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2018å¹´ä¸–ç•Œæ£’çƒç¶“å…¸è³½æ²’æœ‰èˆ‰è¾¦ã€‚\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### æ›´å¤š Prompt ä½¿ç”¨ç¯„ä¾‹: https://platform.openai.com/examples\n",
        "\n"
      ],
      "metadata": {
        "id": "weR_jozkMY7v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## å¦‚ä½•è¨ˆç®— Token æ•¸?\n",
        "\n",
        "é™¤äº† response æœƒå‘Šè¨´ä½ å¯¦éš›ä½¿ç”¨ tokensï¼Œæˆ‘å€‘ä¹Ÿå¯ä»¥è‡ªå·±å…ˆç®—"
      ],
      "metadata": {
        "id": "pqkhXdezfnj5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lpu7kBKZs0ZF",
        "outputId": "fc21ad3b-4e9c-4661-b0f7-a40d57a73993"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.12.25)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.2.2)\n",
            "Installing collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken"
      ],
      "metadata": {
        "id": "B8Tz3-TyfQp-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "string = \"å“ˆå›‰å“ˆå›‰\"\n",
        "\n",
        "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "\n",
        "num_tokens = len(encoding.encode(string))\n",
        "print(num_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJEImmH0s8al",
        "outputId": "4d60e87d-9166-46b1-bc59-11b9d54b2e04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ä½†æ˜¯ Chat API æœƒå†å¤šä¸€é» tokens æ•¸\n",
        "\n",
        "å› ç‚º ChatML çš„åŸå›  https://github.com/openai/openai-python/blob/v0.28.1/chatml.md å¯¦éš›ç”¨é‡æœƒæ›´å¤šä¸€é»\n",
        "\n",
        "å‡ºè™•: https://github.com/openai/openai-cookbook/blob/a74a7a7940928d504f6e9f7300ddef7f47e8659d/examples/How_to_count_tokens_with_tiktoken.ipynb\n"
      ],
      "metadata": {
        "id": "IjlfJw2LlfMu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# å‡ºè‡ª https://platform.openai.com/docs/guides/gpt/managing-tokens\n",
        "def num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0613\"):\n",
        "  \"\"\"Returns the number of tokens used by a list of messages.\"\"\"\n",
        "  try:\n",
        "      encoding = tiktoken.encoding_for_model(model)\n",
        "  except KeyError:\n",
        "      encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "  if model == \"gpt-3.5-turbo-0613\":  # note: future models may deviate from this\n",
        "      num_tokens = 0\n",
        "      for message in messages:\n",
        "          num_tokens += 4  # every message follows <im_start>{role/name}\\n{content}<im_end>\\n\n",
        "          for key, value in message.items():\n",
        "              num_tokens += len(encoding.encode(value))\n",
        "              if key == \"name\":  # if there's a name, the role is omitted\n",
        "                  num_tokens += -1  # role is always required and always 1 token\n",
        "      num_tokens += 2  # every reply is primed with <im_start>assistant\n",
        "      return num_tokens\n",
        "  else:\n",
        "      raise NotImplementedError(f\"\"\"num_tokens_from_messages() is not presently implemented for model {model}.\n",
        "  See https://github.com/openai/openai-python/blob/main/chatml.md for information on how messages are converted to tokens.\"\"\")"
      ],
      "metadata": {
        "id": "heyxTQvY65IC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"hello world!\"\n",
        "    }\n",
        "]\n",
        "\n",
        "num_tokens_from_messages(messages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z9w3t_es66dQ",
        "outputId": "627f2897-be6c-48a0-de1c-d0d6aaaca869"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## æ•…æ„è¶…é token æ•¸é€ æˆéŒ¯èª¤\n",
        "\n",
        "ä¸åŒç¨‹å¼èªè¨€æœ‰ä¸åŒè™•ç†éŒ¯èª¤çš„å¯«æ³•ï¼Œé€™è£¡ Python requests ä¸æœƒä¸Ÿä¾‹å¤–ï¼Œè€Œåƒæ˜¯ Ruby çš„ HTTP library æˆ‘ç¿’æ…£æ˜¯æœƒä¸Ÿä¾‹å¤–çš„....\n",
        "\n",
        "å¦å¤–ä¹Ÿè¦æ³¨æ„ token é™åˆ¶æ˜¯åŒ…æ‹¬ è¼¸å…¥å’Œè¼¸å‡º åŠ åœ¨ä¸€èµ·è¨ˆç®—çš„"
      ],
      "metadata": {
        "id": "Cu8uQYGtl6v5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_message = \"æ•…æ„æ’çˆ†é•·åº¦\" * 4000\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": user_message\n",
        "    }\n",
        "]\n",
        "\n",
        "response = get_completion(messages, temperature=0.7)\n",
        "pp(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fMpEEmxPg4UN",
        "outputId": "10352c7a-6a4a-45a1-9ea4-9978871acbfc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'message': \"This model's maximum context length is 16385 tokens. However, \"\n",
            "            'your messages resulted in 40007 tokens. Please reduce the length '\n",
            "            'of the messages.',\n",
            " 'type': 'invalid_request_error',\n",
            " 'param': 'messages',\n",
            " 'code': 'context_length_exceeded'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## å®˜æ–¹çš„ error codes\n",
        "\n",
        "* https://help.openai.com/en/collections/3808446-api-error-codes-explained\n",
        "* https://platform.openai.com/docs/guides/error-codes/python-library-error-types\n",
        "\n",
        "## Timeout å•é¡Œ\n",
        "\n",
        "gpt-4 æ¯”è¼ƒæ…¢ï¼Œä¸€å€‹ request å¾ˆå¯èƒ½æœƒé•·é”3~5åˆ†é˜ï¼Œæˆ‘ç”šè‡³æœ‰ç´€éŒ„éé”åˆ°25åˆ†é˜çš„.... å› æ­¤å¯ä»¥è€ƒæ…®æŠŠ timeout è¨­é•·ä¸€é»ã€‚\n",
        "\n",
        "* GPT 3.5 Turbo ç´„ 0.02s per tokenï¼Œ4k tokens å¤§ç´„è¦1åˆ†é˜\n",
        "* GPT-4 ç´„ 0.06s per tokenï¼Œ4k tokens å¤§ç´„è¦4åˆ†é˜\n",
        "* GPT-4 Turbo ç´„ 0.04s per tokenï¼Œ4k tokens å¤§ç´„è¦3åˆ†é˜\n",
        "* é›£æ€ª ChatGPT è¦åšæˆ stream ä¸€å€‹å­—ä¸€å€‹å­—å‚³çµ¦ä½ ï¼Œå› ç‚ºç­‰å…¨éƒ¨å®Œæˆæœ‰é»ä¹…å•Š\n",
        "\n",
        "##  server éŒ¯èª¤ https://status.openai.com/\n",
        "\n",
        "ç›¸æ¯” Azure OpenAI, ç›®å‰ OpenAI æ¯”è¼ƒä¸ç©©ä¸€é»ï¼Œæœƒæœ‰è«åçš„ server éŒ¯èª¤....ï¼Œåªèƒ½é‡è©¦ retryã€‚server çš„éŒ¯èª¤è¨Šæ¯é‚„ä¸ä¸€æ¨£ï¼Œä¾‹å¦‚\n",
        "\n",
        "* The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error.\n",
        "* Internal server error\n",
        "* Bad gateway\n",
        "\n",
        "è™•ç† Rate Limit å¯åƒè€ƒ: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_handle_rate_limits.ipynb (é€™æ˜¯ Python çš„ retry è§£æ³•)"
      ],
      "metadata": {
        "id": "QRu6phGUme-Q"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aGynvQFZ3xYc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}