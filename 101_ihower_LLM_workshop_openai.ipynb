{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HSS107048212/LLM_MeiChu/blob/main/101_ihower_LLM_workshop_openai.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "這份 Notebook 示範 OpenAI API 的基本呼叫方式\n",
        "\n",
        "### Google Colab Tips\n",
        "\n",
        "* 用 Shift+Enter 可以執行程式區塊 (或是滑鼠點前面的執行符號)\n",
        "* 如果要修改存檔，需要先點 “檔案\" -> \"在雲端硬碟中儲存副本\"，複製到你自己的目錄下，才可以儲存\n"
      ],
      "metadata": {
        "id": "VaGqfWz7oUVM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 設定 OpenAI API Key 變數\n",
        "\n",
        "請點開左側欄的Key符號，就可以設定 Secret。請設定 openai_api_key\n",
        "\n",
        "2024/5/14 梅竹黑客松 工作坊學員用:\n",
        "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\n"
      ],
      "metadata": {
        "id": "cU4r9MXgfBK8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "9h39wlZGYKSJ"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "openai_api_key = userdata.get('openai_api_key')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "🔥🔥🔥 我們主要使用 requests 和 json library\n",
        "\n",
        "我知道所有教 OpenAI 的 notebook 都用 OpenAI 官方出的 https://github.com/openai/openai-python\n",
        "\n",
        "但是這裡我的目標聽眾包括非 Python 的工程師，因此我只用最最基本的 HTTP library，相信你很容易就可以替換成不同程式語言的 HTTP library。"
      ],
      "metadata": {
        "id": "Mz4OovRbfMIl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json # 這有兩個方法 dumps (物件轉字串) 跟 loads (字串轉物件)\n",
        "from pprint import pp # 為了印出來漂亮"
      ],
      "metadata": {
        "id": "xFHPuW7kYuHu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OpenAI Completion API\n",
        "\n",
        "雖然 OpenAI 後來不太用這種寫法了，不過其他家 LLM 有些是這種 prompt 用法，所以還是認識一下"
      ],
      "metadata": {
        "id": "uEyIUCn7iv-I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "payload = { \"model\": \"gpt-3.5-turbo-instruct\", \"temperature\": 0, \"prompt\": \"講個笑話\" } # 可以改改看 prompt\n",
        "headers = { \"Authorization\": f'Bearer {openai_api_key}', \"Content-Type\": \"application/json\" }\n",
        "# 🐍 Python 的字串，若前面有 f，表示裡面可以用 {variable_name} 來替換變數\n",
        "\n",
        "response = requests.post('https://api.openai.com/v1/completions', headers = headers, data = json.dumps(payload) )\n",
        "obj = json.loads(response.text) # 或是 obj = response.json()\n",
        "pp(obj)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LpVGEJMficmy",
        "outputId": "995db130-c0e8-41d5-ea84-c06574794b43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'id': 'cmpl-9NdFPvWgDE7nwbNnupRU7IVtM7Ep1',\n",
            " 'object': 'text_completion',\n",
            " 'created': 1715419439,\n",
            " 'model': 'gpt-3.5-turbo-instruct',\n",
            " 'choices': [{'text': '\\n\\n有一天，小明去買了一個新的',\n",
            "              'index': 0,\n",
            "              'logprobs': None,\n",
            "              'finish_reason': 'length'}],\n",
            " 'usage': {'prompt_tokens': 7, 'completion_tokens': 16, 'total_tokens': 23}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 呼叫 OpenAI Chat API\n",
        "\n",
        "prompt 參數變成 messages 對話 array 格式\n",
        "\n",
        "role 有分:\n",
        "\n",
        "* system: 定義和定調整個 messages 的行為作用，會放 messages array 的第一個\n",
        "* user: 你的訊息\n",
        "* assistant: AI的回覆 (當你要多輪對話時，就需要將上次AI的回覆放進來)\n",
        "\n",
        "最簡單的形式就是只有一個 user message，把 prompt 全部都放 content 裡面，這樣就等同於 Completion API 用法了。"
      ],
      "metadata": {
        "id": "CMJGh_q8flF0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "payload = { \"model\": \"gpt-3.5-turbo\", \"temperature\": 0,\n",
        "            \"messages\": [ { \"role\": \"user\", \"content\": \"你好\"} ] } # 可以改改看 content\n",
        "headers = { \"Authorization\": f'Bearer {openai_api_key}', \"Content-Type\": \"application/json\" }\n",
        "response = requests.post('https://api.openai.com/v1/chat/completions', headers = headers, data = json.dumps(payload) )\n",
        "obj = json.loads(response.text)\n",
        "\n",
        "pp(obj)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hABW7blVYv5I",
        "outputId": "fb7d800b-c841-4755-9d4f-925ebaa2fe33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'id': 'chatcmpl-9NdFQaiBsEMad3FETSZnACWh6eCFF',\n",
            " 'object': 'chat.completion',\n",
            " 'created': 1715419440,\n",
            " 'model': 'gpt-3.5-turbo-0125',\n",
            " 'choices': [{'index': 0,\n",
            "              'message': {'role': 'assistant', 'content': '你好！有什么可以帮助你的吗？'},\n",
            "              'logprobs': None,\n",
            "              'finish_reason': 'stop'}],\n",
            " 'usage': {'prompt_tokens': 9, 'completion_tokens': 17, 'total_tokens': 26},\n",
            " 'system_fingerprint': None}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "注意到回傳的 reponse 有寫明 model 是 gpt-3-5-turbo-0125 這個版本"
      ],
      "metadata": {
        "id": "XYxjvxyR6MM3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 包成一個輔助函式，之後會常用"
      ],
      "metadata": {
        "id": "-NRdE74nfqNq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_completion(messages, model=\"gpt-3.5-turbo\", temperature=0, max_tokens=300):\n",
        "  payload = { \"model\": model, \"temperature\": temperature, \"messages\": messages, \"max_tokens\": max_tokens }\n",
        "\n",
        "  headers = { \"Authorization\": f'Bearer {openai_api_key}', \"Content-Type\": \"application/json\" }\n",
        "  response = requests.post('https://api.openai.com/v1/chat/completions', headers = headers, data = json.dumps(payload) )\n",
        "  obj = json.loads(response.text)\n",
        "  if response.status_code == 200 :\n",
        "    return obj[\"choices\"][0][\"message\"][\"content\"]\n",
        "  else :\n",
        "    return obj[\"error\"]"
      ],
      "metadata": {
        "id": "pUvYJ90_ZfgI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_message = \"什麼是 Ruby? 請用台灣繁體中文\" # 可以改任意問題\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": user_message\n",
        "    }\n",
        "]\n",
        "\n",
        "response = get_completion(messages, temperature=0)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xvF9fpvJacn3",
        "outputId": "e38a912a-57ad-4044-9566-77b099f531fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ruby 是一種動態、開源的程式語言，由日本程式設計師松本行弘（Matz）所創造。Ruby 被廣泛應用於網頁開發、軟體開發、數據分析等領域，具有簡潔易讀的語法、豐富的內建函式庫和強大的擴充性。Ruby 的特色包括物件導向、函數式編程、動態型別等，被譽為一種優雅而強大的程式語言。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Temperature 溫度\n",
        "\n",
        "可以調看看溫度"
      ],
      "metadata": {
        "id": "5z49Hmohvm-Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_message = \"小明愛吃什麼? \"\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": user_message\n",
        "    }\n",
        "]\n",
        "\n",
        "response = get_completion(messages, temperature=1)  # 可以改看看溫度\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EqU0OYCzvr7j",
        "outputId": "431d83ce-3eca-4369-ff2b-5cd0025bd0af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "這個沒有確切的信息，無法確定小明究竟愛吃什麼。可能要問他本人或者身邊的人才知道。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 使用 System Message\n",
        "\n",
        "只建議在 0613 版本之後使用，之前版本你都放 user message 就好了\n",
        "\n",
        "OpenAI 官方說: 請注意，gpt-3.5-turbo-0301 通常不像 gpt-4-0314 或 gpt-3.5-turbo-0613 那樣密切關注 system message。因此，對於 gpt-3.5-turbo-0301，我們建議將重要指示放在 user message 中。一些開發者發現，在對話變得較長時，將 system prompt 持續地移動到對話的末尾可以防止模型的注意力漸漸偏離。(出自 https://github.com/openai/openai-cookbook/blob/main/examples/How_to_format_inputs_to_ChatGPT_models.ipynb)"
      ],
      "metadata": {
        "id": "Li5om-k2IsrZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 這是 completion 風格(蠻多教材仍這樣寫，包括 ChatGPT Prompt Engineering for Developers 課程)\n",
        "user_message = \"\"\"請分類以下文字是 neutral, negative 或 positive\n",
        "文字: 我覺得這個披薩實在太好吃啦\n",
        "情緒:\n",
        "\"\"\"\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": user_message\n",
        "    }\n",
        "]\n",
        "\n",
        "response = get_completion(messages, temperature=0.7)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2TDaCNkHVnN",
        "outputId": "db067992-564a-47b1-89c8-cbecefebd019"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 可改成使用 system prompt 的風格: 把整體指示放在 system message\n",
        "user_message = \"\"\"\n",
        "文字: 我覺得這個披薩實在太好吃啦\n",
        "\"\"\"\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"請分類以下文字是 neutral, negative 或 positive\"\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": user_message\n",
        "    }\n",
        "]\n",
        "\n",
        "response = get_completion(messages, temperature=0.7)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dvM222w_JIuh",
        "outputId": "98299786-8f18-42d1-e06b-8a7fbc51406b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 連續對話多 messages 的場景示範\n",
        "\n",
        "以下是同一個對話的連續三輪問答"
      ],
      "metadata": {
        "id": "0ukz97tqJqd2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 第一輪問答\n",
        "messages=[\n",
        "      {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "      {\"role\": \"user\", \"content\": \"誰贏得2013年的世界棒球經典賽冠軍?\"},\n",
        "  ]\n",
        "\n",
        "response1 = get_completion(messages, temperature=0.3)\n",
        "print(response1)"
      ],
      "metadata": {
        "id": "bN5teaIdJqPQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e388e80-9e58-4707-dc76-77b8a2ab97d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2013年的世界棒球經典賽冠軍是多明尼加共和國隊。他們在決賽中擊敗了波多黎各隊，贏得了冠軍。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "接著要繼續對話時，你得把 AI 回覆的訊息，放在 role: assistant 裡面一起傳給 OpenAI。因為 API 是無狀態的(Stateless)"
      ],
      "metadata": {
        "id": "DKCM5ERAks4i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 延續同一個對話的 第二輪問答\n",
        "messages=[\n",
        "      {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "      {\"role\": \"user\", \"content\": \"誰贏得2013年的世界棒球經典賽冠軍?\"}, # 這是第一輪的 user 問句\n",
        "      {\"role\": \"assistant\", \"content\": response1 }, # 這是第一輪的 AI 回覆\n",
        "      {\"role\": \"user\", \"content\": \"那2017年呢?\"} # 這是第二輪的 user 問句\n",
        "]\n",
        "\n",
        "response2 = get_completion(messages, temperature=0.3)\n",
        "print(response2)"
      ],
      "metadata": {
        "id": "93XE9p5Fuxl9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa0e14be-4b19-4b38-8310-3d71eea6735d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2017年的世界棒球經典賽冠軍是美國隊。他們在決賽中擊敗了波多黎各隊，贏得了冠軍。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 延續同一個對話的 第三輪問答\n",
        "messages=[\n",
        "      {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "      {\"role\": \"user\", \"content\": \"誰贏得2013年的世界棒球經典賽冠軍?\"}, # 這是第一輪的 user 問句\n",
        "      {\"role\": \"assistant\", \"content\": response1 }, # 這是第一輪的 AI 回覆\n",
        "      {\"role\": \"user\", \"content\": \"那2017年呢?\"}, # 這是第二輪的 user 問句\n",
        "      {\"role\": \"assistant\", \"content\": response2 }, # 這是第二輪的 AI 回覆\n",
        "      {\"role\": \"user\", \"content\": \"美國隊贏過幾次冠軍?\"}, # 這是第三輪的 user 問句\n",
        "]\n",
        "response3 = get_completion(messages, temperature=0.3)\n",
        "print(response3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mrx_x5p8iUSq",
        "outputId": "97f47489-e399-47b8-fe9f-d98b386a2558"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "美國隊在世界棒球經典賽中贏得了一次冠軍，即在2017年。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 模型的幻覺現象 Hallucination"
      ],
      "metadata": {
        "id": "WzPYjxCuR5qG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 如果 第二輪問答時 是問 2018 年\n",
        "messages=[\n",
        "      {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "      {\"role\": \"user\", \"content\": \"誰贏得2013年的世界棒球經典賽冠軍?\"},\n",
        "      {\"role\": \"assistant\", \"content\": \"2013年的世界棒球經典賽冠軍是多明尼加共和國。\"},\n",
        "      {\"role\": \"user\", \"content\": \"那2018年呢?\"}\n",
        "]\n",
        "response = get_completion(messages, temperature=0.3)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "atQnDcZJRr2Z",
        "outputId": "e1775345-287d-4bbd-a164-5f821e64a19d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2018年的世界棒球經典賽冠軍是美國。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 換一種問法\n",
        "messages=[\n",
        "      {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "      {\"role\": \"user\", \"content\": \"誰贏得2013年的世界棒球經典賽冠軍?\"},\n",
        "      {\"role\": \"assistant\", \"content\": \"2013年的世界棒球經典賽冠軍是多明尼加共和國。\"},\n",
        "      {\"role\": \"user\", \"content\": \"那2018年呢? 如果沒舉辦，請回答沒舉辦\"} # 要告訴模型可以回答不知道\n",
        "]\n",
        "response = get_completion(messages, temperature=0.3)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NkbaWB0kR1hK",
        "outputId": "bb8b7ab1-1c19-4fec-8f70-2f4f12066219"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2018年世界棒球經典賽沒有舉辦。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 更多 Prompt 使用範例: https://platform.openai.com/examples\n",
        "\n"
      ],
      "metadata": {
        "id": "weR_jozkMY7v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 如何計算 Token 數?\n",
        "\n",
        "除了 response 會告訴你實際使用 tokens，我們也可以自己先算"
      ],
      "metadata": {
        "id": "pqkhXdezfnj5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lpu7kBKZs0ZF",
        "outputId": "fc21ad3b-4e9c-4661-b0f7-a40d57a73993"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.12.25)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.2.2)\n",
            "Installing collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken"
      ],
      "metadata": {
        "id": "B8Tz3-TyfQp-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "string = \"哈囉哈囉\"\n",
        "\n",
        "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "\n",
        "num_tokens = len(encoding.encode(string))\n",
        "print(num_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJEImmH0s8al",
        "outputId": "4d60e87d-9166-46b1-bc59-11b9d54b2e04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 但是 Chat API 會再多一點 tokens 數\n",
        "\n",
        "因為 ChatML 的原因 https://github.com/openai/openai-python/blob/v0.28.1/chatml.md 實際用量會更多一點\n",
        "\n",
        "出處: https://github.com/openai/openai-cookbook/blob/a74a7a7940928d504f6e9f7300ddef7f47e8659d/examples/How_to_count_tokens_with_tiktoken.ipynb\n"
      ],
      "metadata": {
        "id": "IjlfJw2LlfMu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 出自 https://platform.openai.com/docs/guides/gpt/managing-tokens\n",
        "def num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0613\"):\n",
        "  \"\"\"Returns the number of tokens used by a list of messages.\"\"\"\n",
        "  try:\n",
        "      encoding = tiktoken.encoding_for_model(model)\n",
        "  except KeyError:\n",
        "      encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "  if model == \"gpt-3.5-turbo-0613\":  # note: future models may deviate from this\n",
        "      num_tokens = 0\n",
        "      for message in messages:\n",
        "          num_tokens += 4  # every message follows <im_start>{role/name}\\n{content}<im_end>\\n\n",
        "          for key, value in message.items():\n",
        "              num_tokens += len(encoding.encode(value))\n",
        "              if key == \"name\":  # if there's a name, the role is omitted\n",
        "                  num_tokens += -1  # role is always required and always 1 token\n",
        "      num_tokens += 2  # every reply is primed with <im_start>assistant\n",
        "      return num_tokens\n",
        "  else:\n",
        "      raise NotImplementedError(f\"\"\"num_tokens_from_messages() is not presently implemented for model {model}.\n",
        "  See https://github.com/openai/openai-python/blob/main/chatml.md for information on how messages are converted to tokens.\"\"\")"
      ],
      "metadata": {
        "id": "heyxTQvY65IC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"hello world!\"\n",
        "    }\n",
        "]\n",
        "\n",
        "num_tokens_from_messages(messages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z9w3t_es66dQ",
        "outputId": "627f2897-be6c-48a0-de1c-d0d6aaaca869"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 故意超過 token 數造成錯誤\n",
        "\n",
        "不同程式語言有不同處理錯誤的寫法，這裡 Python requests 不會丟例外，而像是 Ruby 的 HTTP library 我習慣是會丟例外的....\n",
        "\n",
        "另外也要注意 token 限制是包括 輸入和輸出 加在一起計算的"
      ],
      "metadata": {
        "id": "Cu8uQYGtl6v5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_message = \"故意撐爆長度\" * 4000\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": user_message\n",
        "    }\n",
        "]\n",
        "\n",
        "response = get_completion(messages, temperature=0.7)\n",
        "pp(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fMpEEmxPg4UN",
        "outputId": "10352c7a-6a4a-45a1-9ea4-9978871acbfc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'message': \"This model's maximum context length is 16385 tokens. However, \"\n",
            "            'your messages resulted in 40007 tokens. Please reduce the length '\n",
            "            'of the messages.',\n",
            " 'type': 'invalid_request_error',\n",
            " 'param': 'messages',\n",
            " 'code': 'context_length_exceeded'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 官方的 error codes\n",
        "\n",
        "* https://help.openai.com/en/collections/3808446-api-error-codes-explained\n",
        "* https://platform.openai.com/docs/guides/error-codes/python-library-error-types\n",
        "\n",
        "## Timeout 問題\n",
        "\n",
        "gpt-4 比較慢，一個 request 很可能會長達3~5分鐘，我甚至有紀錄過達到25分鐘的.... 因此可以考慮把 timeout 設長一點。\n",
        "\n",
        "* GPT 3.5 Turbo 約 0.02s per token，4k tokens 大約要1分鐘\n",
        "* GPT-4 約 0.06s per token，4k tokens 大約要4分鐘\n",
        "* GPT-4 Turbo 約 0.04s per token，4k tokens 大約要3分鐘\n",
        "* 難怪 ChatGPT 要做成 stream 一個字一個字傳給你，因為等全部完成有點久啊\n",
        "\n",
        "##  server 錯誤 https://status.openai.com/\n",
        "\n",
        "相比 Azure OpenAI, 目前 OpenAI 比較不穩一點，會有莫名的 server 錯誤....，只能重試 retry。server 的錯誤訊息還不一樣，例如\n",
        "\n",
        "* The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error.\n",
        "* Internal server error\n",
        "* Bad gateway\n",
        "\n",
        "處理 Rate Limit 可參考: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_handle_rate_limits.ipynb (這是 Python 的 retry 解法)"
      ],
      "metadata": {
        "id": "QRu6phGUme-Q"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aGynvQFZ3xYc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}